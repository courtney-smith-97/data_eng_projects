{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Tracking User Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to be run in a pyspark container. The environment is set up using the `docker-compose.yml` file in this repo and by running the following steps in the terminal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start container and pull images:  \n",
    "`docker-compose up -d`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create topic called assessments to store assessment data:  \n",
    "`docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link spark back to main directory:  \n",
    "`docker-compose exec spark ln -s /w205 w205`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data folder and download data to it:   \n",
    "`mkdir data`  \n",
    "`curl -L -o data/assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish data to our assessments topic:  \n",
    "`docker-compose exec mids bash -c \"cat /w205/project-2-courtney-smith-97/data/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to pyspark and launch a jupyter notebook:  \n",
    "`docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark`    \n",
    "\n",
    "To access the notebook, copy the URL and replace '0.0.0.0' with the external IP address of the GCP VM instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing and Querying the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we read the data from our assessments topic and create an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load()\n",
    "raw_assessments.cache()\n",
    "raw_assessments.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`value` contains the data about the assessments. The next step is to extract this component and create a temporary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "#select only value component\n",
    "val_assessments = raw_assessments.select(raw_assessments.value.cast('string'))\n",
    "#create temporary table from df\n",
    "extract_assessments = val_assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "extract_assessments.registerTempTable('assessments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run queries against the temporary table `assessments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|           sequences|          started_at|        user_exam_id|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|Map(questions -> ...|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|Map(questions -> ...|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|Map(questions -> ...|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|Map(questions -> ...|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|Map(questions -> ...|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#look at the first few rows of data\n",
    "spark.sql('select * from assessments limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|count(DISTINCT base_exam_id)|\n",
      "+----------------------------+\n",
      "|                         107|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#count the number of distinct exams taken\n",
    "spark.sql('select count(distinct(base_exam_id)) from assessments').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|           exam_name|num_taken|\n",
      "+--------------------+---------+\n",
      "|        Learning Git|      394|\n",
      "|Introduction to P...|      162|\n",
      "|Intermediate Pyth...|      158|\n",
      "|Introduction to J...|      158|\n",
      "|Learning to Progr...|      128|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show the 5 most common exams\n",
    "spark.sql('select exam_name, count(*) as num_taken from assessments group by exam_name order by num_taken desc limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the average score for the 5 most common exams\n",
    "\n",
    "#create function to extract total number of questions and number of correct answers from sequences nested dict\n",
    "def map_correct(assessments):\n",
    "    raw_dict = json.loads(assessments.value)\n",
    "    assessment_counts = []\n",
    "    if \"sequences\" in raw_dict:        \n",
    "        if \"counts\" in raw_dict[\"sequences\"]:   \n",
    "            if \"correct\" in raw_dict[\"sequences\"][\"counts\"] and \"total\" in raw_dict[\"sequences\"][\"counts\"]:   \n",
    "                seq_dict = {\"keen_id\" : raw_dict[\"keen_id\"],\n",
    "                           \"num_correct\" : raw_dict[\"sequences\"][\"counts\"][\"correct\"], \n",
    "                           \"total\" : raw_dict[\"sequences\"][\"counts\"][\"total\"]}\n",
    "                assessment_counts.append(Row(**seq_dict))\n",
    "    return assessment_counts\n",
    "#create df with id, num correct, and total questions\n",
    "correct_total = val_assessments.rdd.flatMap(map_correct).toDF()\n",
    "correct_total.registerTempTable('correct_counts')\n",
    "\n",
    "#join new df to assessments on id, get avg exam score for top 5 most common exams\n",
    "spark.sql(\"select assessments.exam_name, \" +\n",
    "          \"count(*) as num_taken, \"+\n",
    "          \"round(avg(correct_counts.num_correct*100 / correct_counts.total),2) as avg_score \" +\n",
    "          \"from assessments, correct_counts where assessments.keen_id = correct_counts.keen_id \" +\n",
    "          \"group by assessments.exam_name \" +\n",
    "          \"order by avg_score desc \" +\n",
    "          \"limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data and shut down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write extracted assessment data to \n",
    "extract_assessments.write.parquet(\"/tmp/extract_assessments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `docker-compose exec cloudera hadoop fs -ls /tmp/extract_assessments` to check that the file saved, then shut down the kernel and run `docker-compose down` in the terminal to stop the container."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m78"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
